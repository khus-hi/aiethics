#!/usr/bin/env python3



import openai
import json
import sys
import argparse
from pathlib import Path
from typing import Optional, Dict, List

# Import for different file formats
import PyPDF2
from docx import Document


# ============================================================================
# FILE READING FUNCTIONS
# ============================================================================

def read_pdf(file_path: Path) -> Optional[str]:
    """Extract text from a PDF file."""
    try:
        text = ""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            num_pages = len(pdf_reader.pages)
            
            print(f"  Reading {num_pages} pages...")
            for page_num in range(num_pages):
                page = pdf_reader.pages[page_num]
                text += page.extract_text()
        
        return text.strip()
    except Exception as e:
        print(f"  Error reading PDF: {e}")
        return None


def read_docx(file_path: Path) -> Optional[str]:
    """Extract text from a DOCX file."""
    try:
        doc = Document(file_path)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text.strip()
    except Exception as e:
        print(f"  Error reading DOCX: {e}")
        return None


def read_txt(file_path: Path) -> Optional[str]:
    """Read text from a TXT file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read().strip()
    except UnicodeDecodeError:
        # Try with different encoding
        try:
            with open(file_path, 'r', encoding='latin-1') as file:
                return file.read().strip()
        except Exception as e:
            print(f"  Error reading TXT: {e}")
            return None
    except Exception as e:
        print(f"  Error reading TXT: {e}")
        return None


def read_model_card(file_path: Path) -> Optional[str]:
    """
    Read model card from various file formats.
    Supports: .pdf, .txt, .doc, .docx
    """
    if not file_path.exists():
        print(f"Error: File '{file_path}' not found.")
        return None
    
    file_extension = file_path.suffix.lower()
    
    print(f"Reading {file_extension} file: {file_path.name}")
    
    if file_extension == '.pdf':
        return read_pdf(file_path)
    elif file_extension in ['.docx', '.doc']:
        return read_docx(file_path)
    elif file_extension == '.txt':
        return read_txt(file_path)
    else:
        print(f"Unsupported file format: {file_extension}")
        print("Supported formats: .pdf, .txt, .doc, .docx")
        return None


# ============================================================================
# EVALUATION PROMPT
# ============================================================================

EVALUATION_PROMPT = """You are an AI transparency and governance expert. Evaluate the following model card against the Transparency Level 4 (L4) standard.

**Transparency L4 Standard Definition:**
System and model cards and safety reports are published and versioned. This includes:
1. Published model cards with comprehensive technical and safety information
2. Published safety reports (can be integrated or separate)
3. Clear versioning with version numbers
4. Documentation of updates and changes over time
5. Publicly accessible and discoverable

**Evaluation Framework:**

For each criterion below, provide:
- Score: 0 (Not Met), 1 (Partially Met), 2 (Fully Met)
- Evidence: Specific quotes or observations from the document
- Gaps: What's missing or could be improved

**Criteria to Evaluate:**

1. **Publication Status**
   - Is the model card publicly published?
   - Is there a clear publication date?
   - Is it easily discoverable?

2. **Comprehensiveness of Model Card**
   - Model architecture and capabilities
   - Training data information
   - Evaluation results and benchmarks
   - Intended use cases
   - Known limitations
   - Acceptable use policies

3. **Safety Reporting**
   - Safety evaluation results included
   - Risk assessments documented
   - Mitigation strategies described
   - Red teaming results shared
   - Frontier safety evaluations (if applicable)

4. **Versioning**
   - Explicit version number present (e.g., v1.0, v2.1)
   - Publication/release date clearly stated
   - References to versioning policy

5. **Change Documentation**
   - Changelog or update history
   - Documentation of what changed between versions
   - Process for future updates described

6. **Accessibility and Format**
   - Clear structure and organization
   - Professional formatting
   - References to additional resources
   - Machine-readable elements (if applicable)

**Output Format:**

Provide your evaluation in the following JSON structure:

```json
{{
  "overall_score": "X/12",
  "overall_rating": "Does Not Meet L4 / Partially Meets L4 / Fully Meets L4",
  "criteria_scores": {{
    "publication_status": {{"score": X, "evidence": "...", "gaps": "..."}},
    "comprehensiveness": {{"score": X, "evidence": "...", "gaps": "..."}},
    "safety_reporting": {{"score": X, "evidence": "...", "gaps": "..."}},
    "versioning": {{"score": X, "evidence": "...", "gaps": "..."}},
    "change_documentation": {{"score": X, "evidence": "...", "gaps": "..."}},
    "accessibility": {{"score": X, "evidence": "...", "gaps": "..."}}
  }},
  "strengths": ["strength 1", "strength 2", "..."],
  "weaknesses": ["weakness 1", "weakness 2", "..."],
  "recommendations": ["recommendation 1", "recommendation 2", "..."],
  "summary": "2-3 sentence overall assessment"
}}
```

**Model Card to Evaluate:**

{model_card_content}

Provide a thorough, evidence-based evaluation."""


# ============================================================================
# EVALUATION FUNCTIONS
# ============================================================================

def evaluate_model_card(model_card_content: str, model: str = "gpt-4o", 
                       temperature: float = 0.2) -> Optional[Dict]:
    """
    Evaluate a model card using OpenAI's API.
    
    Args:
        model_card_content: The full text of the model card
        model: OpenAI model to use
        temperature: Temperature for generation
    
    Returns:
        Evaluation results dictionary
    """
    try:
        full_prompt = EVALUATION_PROMPT.format(model_card_content=model_card_content)
        
        print(f"  Calling OpenAI API ({model})...")
        response = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an AI transparency and governance expert specializing in evaluating AI model documentation against industry standards."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=temperature,
            max_tokens=4000
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse JSON from response
        try:
            if "```json" in evaluation_text:
                json_start = evaluation_text.find("```json") + 7
                json_end = evaluation_text.find("```", json_start)
                json_str = evaluation_text[json_start:json_end].strip()
                evaluation_result = json.loads(json_str)
            else:
                evaluation_result = json.loads(evaluation_text)
        except json.JSONDecodeError:
            evaluation_result = {
                "raw_evaluation": evaluation_text,
                "note": "Could not parse as JSON, returning raw text"
            }
        
        return evaluation_result
        
    except Exception as e:
        print(f"  Error during API call: {e}")
        return None


def save_evaluation(evaluation_result: Dict, output_file: Path) -> None:
    """Save evaluation results to a JSON file."""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ Saved to {output_file}")
    except Exception as e:
        print(f"  Error saving evaluation: {e}")


def print_summary(evaluation_result: Dict, file_name: str = "") -> None:
    """Print a formatted summary of the evaluation."""
    if "raw_evaluation" in evaluation_result:
        print("\n" + "="*80)
        print("EVALUATION RESULT")
        print("="*80)
        print(evaluation_result["raw_evaluation"])
        return
    
    print("\n" + "="*80)
    if file_name:
        print(f"EVALUATION SUMMARY: {file_name}")
    else:
        print("EVALUATION SUMMARY")
    print("="*80)
    
    print(f"\nðŸ“Š Overall Score: {evaluation_result.get('overall_score', 'N/A')}")
    print(f"ðŸ“ˆ Overall Rating: {evaluation_result.get('overall_rating', 'N/A')}")
    
    print("\n" + "-"*80)
    print("CRITERIA SCORES")
    print("-"*80)
    for criterion, details in evaluation_result.get('criteria_scores', {}).items():
        score = details.get('score', 0)
        status = "âœ…" if score == 2 else ("âš ï¸ " if score == 1 else "âŒ")
        print(f"\n{status} {criterion.replace('_', ' ').title()}: {score}/2")
        if details.get('gaps'):
            print(f"   Gap: {details.get('gaps', 'N/A')[:100]}...")
    
    print("\n" + "-"*80)
    print("STRENGTHS")
    print("-"*80)
    for strength in evaluation_result.get('strengths', []):
        print(f"  âœ“ {strength}")
    
    print("\n" + "-"*80)
    print("WEAKNESSES")
    print("-"*80)
    for weakness in evaluation_result.get('weaknesses', []):
        print(f"  âœ— {weakness}")
    
    print("\n" + "-"*80)
    print("RECOMMENDATIONS")
    print("-"*80)
    for rec in evaluation_result.get('recommendations', []):
        print(f"  â†’ {rec}")
    
    print("\n" + "-"*80)
    print("SUMMARY")
    print("-"*80)
    print(evaluation_result.get('summary', 'N/A'))
    print("\n" + "="*80 + "\n")


# ============================================================================
# BATCH PROCESSING
# ============================================================================

def process_file(file_path: Path, model: str, temperature: float, 
                output_dir: Path, verbose: bool = True) -> Optional[Dict]:
    """Process a single file."""
    print(f"\n{'='*80}")
    print(f"Processing: {file_path.name}")
    print('='*80)
    
    # Read file
    content = read_model_card(file_path)
    if not content:
        print(f"  âœ— Failed to read file")
        return None
    
    print(f"  âœ“ Loaded {len(content):,} characters, {len(content.split()):,} words")
    
    # Evaluate
    result = evaluate_model_card(content, model, temperature)
    if not result:
        print(f"  âœ— Evaluation failed")
        return None
    
    print(f"  âœ“ Evaluation complete")
    
    # Save
    output_file = output_dir / f"{file_path.stem}_evaluation.json"
    save_evaluation(result, output_file)
    
    # Print summary
    if verbose:
        print_summary(result, file_path.name)
    else:
        score = result.get('overall_score', 'N/A')
        rating = result.get('overall_rating', 'N/A')
        print(f"  Score: {score} | Rating: {rating}")
    
    return result


def batch_process(file_paths: List[Path], model: str, temperature: float, 
                 output_dir: Path) -> Dict[str, Dict]:
    """Process multiple files."""
    results = {}
    
    print(f"\n{'#'*80}")
    print(f"BATCH PROCESSING: {len(file_paths)} files")
    print('#'*80)
    
    for i, file_path in enumerate(file_paths, 1):
        print(f"\n[{i}/{len(file_paths)}]")
        result = process_file(file_path, model, temperature, output_dir, verbose=False)
        if result:
            results[file_path.name] = result
    
    # Print comparison summary
    print(f"\n{'#'*80}")
    print("BATCH SUMMARY")
    print('#'*80)
    print(f"\n{'File':<40} {'Score':<10} {'Rating':<20}")
    print("-"*80)
    for filename, result in results.items():
        score = result.get('overall_score', 'N/A')
        rating = result.get('overall_rating', 'N/A')
        print(f"{filename:<40} {score:<10} {rating:<20}")
    
    return results


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Evaluate AI model cards against Transparency L4 standard",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python transparency_evaluator.py model_card.pdf
  python transparency_evaluator.py model_card.txt --model gpt-4-turbo
  python transparency_evaluator.py *.pdf --batch
  python transparency_evaluator.py file1.pdf file2.docx --batch --output results/
        """
    )
    
    parser.add_argument('files', nargs='+', help='Model card file(s) to evaluate')
    parser.add_argument('--model', default='gpt-4o', 
                       choices=['gpt-4o', 'gpt-4-turbo', 'gpt-3.5-turbo'],
                       help='OpenAI model to use (default: gpt-4o)')
    parser.add_argument('--temperature', type=float, default=0.2,
                       help='Temperature for generation (default: 0.2)')
    parser.add_argument('--api-key', help='OpenAI API key (or set OPENAI_API_KEY env var)')
    parser.add_argument('--output', '-o', default='.',
                       help='Output directory for results (default: current directory)')
    parser.add_argument('--batch', action='store_true',
                       help='Batch mode: process multiple files with summary')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='Quiet mode: minimal output')
    
    args = parser.parse_args()
    
    # Setup API key
    if args.api_key:
        openai.api_key = args.api_key
    elif not openai.api_key:
        print("Error: OpenAI API key not found.")
        print("Set OPENAI_API_KEY environment variable or use --api-key")
        sys.exit(1)
    
    # Setup output directory
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Convert file arguments to Path objects
    file_paths = [Path(f) for f in args.files]
    
    # Validate files exist
    missing_files = [f for f in file_paths if not f.exists()]
    if missing_files:
        print("Error: The following files were not found:")
        for f in missing_files:
            print(f"  - {f}")
        sys.exit(1)
    
    # Process files
    try:
        if len(file_paths) > 1 or args.batch:
            batch_process(file_paths, args.model, args.temperature, output_dir)
        else:
            process_file(file_paths[0], args.model, args.temperature, 
                        output_dir, verbose=not args.quiet)
        
        print("\nâœ“ All evaluations complete!")
        
    except KeyboardInterrupt:
        print("\n\nInterrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâœ— Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
